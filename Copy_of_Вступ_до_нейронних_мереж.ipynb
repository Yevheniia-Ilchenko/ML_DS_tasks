{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yevheniia-Ilchenko/ML_DS_tasks/blob/main/Copy_of_%D0%92%D1%81%D1%82%D1%83%D0%BF_%D0%B4%D0%BE_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B8%D1%85_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(inputs)\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0125b00-66a7-4816-ef68-b61377197961"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = torch.tensor(targets)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0UheLGm4Hri",
        "outputId": "097c8038-e30f-47cb-b697-7dbc0bf25391"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ead365a-ef98-411d-b6a7-112d3ae4eebb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5e38320d30>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_feature = inputs.shape[1]\n",
        "print(n_feature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUy2Taut7OXT",
        "outputId": "e0f64493-9ea0-436e-ec49-d98fdd24d514"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.rand(n_feature, 1, requires_grad=True)\n",
        "b = torch.rand(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"w =\", w)\n",
        "print(\"b =\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqCXlYxV8WZB",
        "outputId": "8dbd1109-25c5-4200-d8c3-1b17337c435f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w = tensor([[0.7576],\n",
            "        [0.2793],\n",
            "        [0.4031]], requires_grad=True)\n",
            "b = tensor([0.7347], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(w, b, x):\n",
        "  z = x @ w.T + b\n",
        "  y_hat = torch.sigmoid(z)\n",
        "  return y_hat"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = model(w,b, inputs)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w2h1lAi-Gme",
        "outputId": "739d979a-a3dd-40a6-a591-fe656836f613"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Підозріло бо всі числа однакові і модель впевнена на 100%, бо ми не тренували модель на розмічених даних\n"
      ],
      "metadata": {
        "id": "O6ICTgRL_eEs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі.  L(y,y^)=−[y⋅log(y^)+(1−y)⋅log(1−y^)]"
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    loss = - (true_labels * torch.log(predicted_probs) + (1 - true_labels) * torch.log(1 - predicted_probs))\n",
        "\n",
        "    return torch.mean(loss)"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW6x-u98Fe5h",
        "outputId": "b86ce4a7-e871-43d4-aa19-c760f6e8f469"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(y, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26Mx62vAwEh",
        "outputId": "e2a54ef7-7686-4d24-f9ac-b959c20c8c37"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(nan, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quX38fKtFOLU",
        "outputId": "6e567cb3-594f-4496-fd8e-38522ac9091f"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[nan],\n",
            "        [nan],\n",
            "        [nan]])\n",
            "tensor([nan])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "я вважаю проблема в значенні 1"
      ],
      "metadata": {
        "id": "bsSHXMgrF96K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzQx_yozIa-k",
        "outputId": "0cb7d2ff-7693-49e3-f07d-f50315729b4c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "tensor([0.0006], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = model(w,b, inputs)\n",
        "print(y)\n"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0a0bdfe-1cfa-4480-edfd-cca944044d71"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy(y, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_sdO1mDHHqp",
        "outputId": "cca60469-66e2-42a1-b148-a53ae47a9b10"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6829, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odj57nY4HvYz",
        "outputId": "95bbe234-9d42-4122-8f07-0da33b7b5bce"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()"
      ],
      "metadata": {
        "id": "qzpLNdfqKt1u"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    y_pred = model(w, b, inputs)\n",
        "    loss = binary_cross_entropy(y_pred, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= 1e-5 * w.grad\n",
        "        b -= 1e-5 * b.grad\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()"
      ],
      "metadata": {
        "id": "mObHPyE06qsO"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(w, b,inputs)\n",
        "loss =binary_cross_entropy(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIWQx1EvKByf",
        "outputId": "994995d1-73c4-40e5-b474-3318c783b152"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.3199, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT4MQ5-PKBwV",
        "outputId": "9dd3c1c7-87f5-4e41-e516-636aa2b75656"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5714],\n",
              "        [0.6689],\n",
              "        [0.9226],\n",
              "        [0.1322],\n",
              "        [0.8800]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFMxMnqhKBuH",
        "outputId": "2b99a504-7971-4e96-f704-0259dfdf8af9"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "якщо правильно обрати трешхолд, то визначення можна вважати точними"
      ],
      "metadata": {
        "id": "x_wQb61mMTS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDC6Wc8-NAtF",
        "outputId": "099f00b2-f847-427c-b8a2-ffcefe7e93f5"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c97413-cde8-4674-db0e-738d99a86180"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.],\n",
              "         [102.,  43.,  37.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [0.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogReg(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(input_dim, 1)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear(x)\n",
        "    x = self.sig(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogReg(input_dim=3)"
      ],
      "metadata": {
        "id": "WOq-X6RZPAvL"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import binary_cross_entropy\n",
        "\n",
        "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "loss_fn = binary_cross_entropy\n",
        "\n"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(model(inputs), targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BxjsqDTUaZy",
        "outputId": "8b6f8ee7-a773-4766-ec13-455b4acf0369"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.0783, grad_fn=<BinaryCrossEntropyBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKTKLOxYRekk"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "    print(param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGoeulDnXJgz",
        "outputId": "56796d68-25ba-440c-8604-c4e8b6866d2a"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(targets.min(), targets.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDVpUan7W90g",
        "outputId": "383ee9d3-e2da-4b39-afdd-bc1622ee4965"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.) tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cEHQH9qE626k"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt3lRwIQUAAe",
        "outputId": "b5d5ceb3-8be9-470b-95ac-202c52b3bba2"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function binary_cross_entropy at 0x7f5e302d5300>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG5Dg-OTSGP6",
        "outputId": "ae7a9062-2112-401b-9587-17dd43ccd925"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 1.9190\n",
            "Epoch [20/1000], Loss: 1.8073\n",
            "Epoch [30/1000], Loss: 1.7335\n",
            "Epoch [40/1000], Loss: 1.6714\n",
            "Epoch [50/1000], Loss: 1.6224\n",
            "Epoch [60/1000], Loss: 1.5623\n",
            "Epoch [70/1000], Loss: 1.5102\n",
            "Epoch [80/1000], Loss: 1.4556\n",
            "Epoch [90/1000], Loss: 1.4085\n",
            "Epoch [100/1000], Loss: 1.3583\n",
            "Epoch [110/1000], Loss: 1.3096\n",
            "Epoch [120/1000], Loss: 1.2594\n",
            "Epoch [130/1000], Loss: 1.2134\n",
            "Epoch [140/1000], Loss: 1.1737\n",
            "Epoch [150/1000], Loss: 1.1251\n",
            "Epoch [160/1000], Loss: 1.0860\n",
            "Epoch [170/1000], Loss: 1.0469\n",
            "Epoch [180/1000], Loss: 1.0097\n",
            "Epoch [190/1000], Loss: 0.9737\n",
            "Epoch [200/1000], Loss: 0.9395\n",
            "Epoch [210/1000], Loss: 0.9039\n",
            "Epoch [220/1000], Loss: 0.8747\n",
            "Epoch [230/1000], Loss: 0.8478\n",
            "Epoch [240/1000], Loss: 0.8224\n",
            "Epoch [250/1000], Loss: 0.8016\n",
            "Epoch [260/1000], Loss: 0.7684\n",
            "Epoch [270/1000], Loss: 0.7460\n",
            "Epoch [280/1000], Loss: 0.7270\n",
            "Epoch [290/1000], Loss: 0.7072\n",
            "Epoch [300/1000], Loss: 0.6881\n",
            "Epoch [310/1000], Loss: 0.6706\n",
            "Epoch [320/1000], Loss: 0.6540\n",
            "Epoch [330/1000], Loss: 0.6420\n",
            "Epoch [340/1000], Loss: 0.6254\n",
            "Epoch [350/1000], Loss: 0.6119\n",
            "Epoch [360/1000], Loss: 0.6018\n",
            "Epoch [370/1000], Loss: 0.5884\n",
            "Epoch [380/1000], Loss: 0.5797\n",
            "Epoch [390/1000], Loss: 0.5683\n",
            "Epoch [400/1000], Loss: 0.5604\n",
            "Epoch [410/1000], Loss: 0.5527\n",
            "Epoch [420/1000], Loss: 0.5419\n",
            "Epoch [430/1000], Loss: 0.5339\n",
            "Epoch [440/1000], Loss: 0.5287\n",
            "Epoch [450/1000], Loss: 0.5229\n",
            "Epoch [460/1000], Loss: 0.5152\n",
            "Epoch [470/1000], Loss: 0.5072\n",
            "Epoch [480/1000], Loss: 0.5013\n",
            "Epoch [490/1000], Loss: 0.4963\n",
            "Epoch [500/1000], Loss: 0.4904\n",
            "Epoch [510/1000], Loss: 0.4860\n",
            "Epoch [520/1000], Loss: 0.4806\n",
            "Epoch [530/1000], Loss: 0.4766\n",
            "Epoch [540/1000], Loss: 0.4715\n",
            "Epoch [550/1000], Loss: 0.4705\n",
            "Epoch [560/1000], Loss: 0.4632\n",
            "Epoch [570/1000], Loss: 0.4613\n",
            "Epoch [580/1000], Loss: 0.4569\n",
            "Epoch [590/1000], Loss: 0.4543\n",
            "Epoch [600/1000], Loss: 0.4552\n",
            "Epoch [610/1000], Loss: 0.4476\n",
            "Epoch [620/1000], Loss: 0.4431\n",
            "Epoch [630/1000], Loss: 0.4420\n",
            "Epoch [640/1000], Loss: 0.4386\n",
            "Epoch [650/1000], Loss: 0.4340\n",
            "Epoch [660/1000], Loss: 0.4345\n",
            "Epoch [670/1000], Loss: 0.4300\n",
            "Epoch [680/1000], Loss: 0.4274\n",
            "Epoch [690/1000], Loss: 0.4258\n",
            "Epoch [700/1000], Loss: 0.4231\n",
            "Epoch [710/1000], Loss: 0.4204\n",
            "Epoch [720/1000], Loss: 0.4195\n",
            "Epoch [730/1000], Loss: 0.4184\n",
            "Epoch [740/1000], Loss: 0.4190\n",
            "Epoch [750/1000], Loss: 0.4118\n",
            "Epoch [760/1000], Loss: 0.4100\n",
            "Epoch [770/1000], Loss: 0.4076\n",
            "Epoch [780/1000], Loss: 0.4081\n",
            "Epoch [790/1000], Loss: 0.4039\n",
            "Epoch [800/1000], Loss: 0.4030\n",
            "Epoch [810/1000], Loss: 0.4014\n",
            "Epoch [820/1000], Loss: 0.4024\n",
            "Epoch [830/1000], Loss: 0.3981\n",
            "Epoch [840/1000], Loss: 0.3952\n",
            "Epoch [850/1000], Loss: 0.3944\n",
            "Epoch [860/1000], Loss: 0.3924\n",
            "Epoch [870/1000], Loss: 0.3937\n",
            "Epoch [880/1000], Loss: 0.3915\n",
            "Epoch [890/1000], Loss: 0.3885\n",
            "Epoch [900/1000], Loss: 0.3869\n",
            "Epoch [910/1000], Loss: 0.3871\n",
            "Epoch [920/1000], Loss: 0.3849\n",
            "Epoch [930/1000], Loss: 0.3830\n",
            "Epoch [940/1000], Loss: 0.3842\n",
            "Epoch [950/1000], Loss: 0.3810\n",
            "Epoch [960/1000], Loss: 0.3849\n",
            "Epoch [970/1000], Loss: 0.3792\n",
            "Epoch [980/1000], Loss: 0.3778\n",
            "Epoch [990/1000], Loss: 0.3760\n",
            "Epoch [1000/1000], Loss: 0.3758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "bFx-4GsLSCTC",
        "outputId": "5756aff1-48ec-41c9-9735-941afee8cfe0"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}